# -*- coding: utf-8 -*-
"""Indic_Dataset_Bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PK8bxM5bxh1xekyzzZSXXVlBjKPkaEc7
"""

!pip install transformers datasets torch scikit-learn pandas matplotlib seaborn tqdm

from google.colab import drive
drive.mount('/content/drive')

import torch
print("PyTorch version:", torch.__version__)
print("GPU available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU name:", torch.cuda.get_device_name(0))

import os, pandas as pd

base = "/content/drive/MyDrive/Machine_Learning_Mini_Project"


for name in ["English_train_lpc.xlsx", "English_valid_lpc.xlsx", "English_test_lpc.xlsx"]:
    path = os.path.join(base, name)
    print(f"\nChecking: {path}")
    print("Exists:", os.path.exists(path))
    if os.path.exists(path):
        df = pd.read_excel(path)
        print("Rows:", len(df), "| Columns:", df.columns.tolist())
        print(df.head(2))

import pandas as pd
import os

# Define paths
base_path = "/content/drive/MyDrive/Machine_Learning_Mini_Project"

train_path = os.path.join(base_path, "English_train_lpc.xlsx")
valid_path = os.path.join(base_path, "English_valid_lpc.xlsx")
test_path  = os.path.join(base_path, "English_test_lpc.xlsx")

# Read Excel files
train_df = pd.read_excel(train_path)
val_df   = pd.read_excel(valid_path)
test_df  = pd.read_excel(test_path)

# Show few samples
print("Train shape:", train_df.shape)
print("Validation shape:", val_df.shape)
print("Test shape:", test_df.shape)

train_df.head()

import re

def clean_text(text):
    text = str(text)
    text = re.sub(r"http\S+", "", text)  # remove URLs
    text = re.sub(r"<.*?>", "", text)    # remove HTML tags
    text = re.sub(r"\s+", " ", text)     # collapse spaces
    return text.strip()

for df in [train_df, val_df, test_df]:
    df['text'] = df['text'].apply(clean_text)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

train_df['label_id'] = le.fit_transform(train_df['labels'])
val_df['label_id']   = le.transform(val_df['labels'])
test_df['label_id']  = le.transform(test_df['labels'])

num_classes = len(le.classes_)
print("Number of unique classes:", num_classes)
print("Classes:", list(le.classes_))

train_df['labels'].value_counts().plot(kind='bar', figsize=(10,4), title='Label Distribution - Train')

print("Null values per column:\n", train_df.isnull().sum())
print("Empty text rows:", (train_df['text'].str.strip() == '').sum())

# Remove rows where label is NaN
train_df = train_df.dropna(subset=['labels'])
val_df = val_df.dropna(subset=['labels'])
test_df = test_df.dropna(subset=['labels'])

print("Train shape after dropping NaN labels:", train_df.shape)
print("Validation shape after dropping NaN labels:", val_df.shape)
print("Test shape after dropping NaN labels:", test_df.shape)


# Re-check unique classes (should now be 12)
num_classes = len(train_df['labels'].unique())
print("Unique class count after cleaning:", num_classes)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
train_df['label_id'] = le.fit_transform(train_df['labels'])
val_df['label_id']   = le.transform(val_df['labels'])
test_df['label_id']  = le.transform(test_df['labels'])

num_classes = len(le.classes_)
print("Final classes:", list(le.classes_))

from transformers import AutoTokenizer

model_name = "bert-base-uncased"  # main English model
tokenizer = AutoTokenizer.from_pretrained(model_name)

from datasets import Dataset

# Convert DataFrames to Hugging Face Datasets
train_dataset = Dataset.from_pandas(train_df[['text', 'label_id']])
val_dataset   = Dataset.from_pandas(val_df[['text', 'label_id']])
test_dataset  = Dataset.from_pandas(test_df[['text', 'label_id']])

# Tokenization function
def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        truncation=True,
        padding='max_length',
        max_length=256
    )

# Apply tokenization
train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset   = val_dataset.map(tokenize_function, batched=True)
test_dataset  = test_dataset.map(tokenize_function, batched=True)

# Rename label_id column to 'labels' for Trainer API
train_dataset = train_dataset.rename_column("label_id", "labels")
val_dataset   = val_dataset.rename_column("label_id", "labels")
test_dataset  = test_dataset.rename_column("label_id", "labels")

# Set tensor format
train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
val_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

print(train_dataset[0])
print("Decoded text sample:", tokenizer.decode(train_dataset[0]['input_ids']))

from transformers import AutoModelForSequenceClassification

model_name = "bert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)

import numpy as np
from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='macro')
    return {"accuracy": acc, "f1": f1}

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    greater_is_better=True,
    logging_dir='./logs',
    logging_strategy="epoch",
    report_to="none" # Explicitly disable wandb reporting
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

import os
os.environ["WANDB_DISABLED"] = "true"
print("WANDB logging is disabled.")

trainer.train()

metrics = trainer.evaluate(test_dataset)
print("Test set performance:")
print(metrics)

import torch

def predict_category(text, model, tokenizer, label_encoder):
    # Put model in evaluation mode
    model.eval()
    # Tokenize user input
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=256
    ).to(model.device)

    # Forward pass
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_class_id = logits.argmax(dim=-1).item()

    # Decode label
    predicted_label = label_encoder.inverse_transform([predicted_class_id])[0]
    return predicted_label

examples = [
    "India won the final match of the T20 World Cup after an outstanding performance by the bowlers.",
    "The stock market surged to a record high as major technology companies posted strong earnings.",
    "The government announced new educational reforms for rural schools."
]

for text in examples:
    category = predict_category(text, model, tokenizer, le)
    print(f"\nüì∞ Text: {text}\n‚û°Ô∏è Predicted Category: {category}")

