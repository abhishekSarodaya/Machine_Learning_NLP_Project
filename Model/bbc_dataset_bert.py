# -*- coding: utf-8 -*-
"""BBC_Dataset_bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1--SEzLc1y9EcWe_Xzn7GFyJbZSOK3br5
"""

!pip install transformers datasets torch scikit-learn pandas matplotlib seaborn tqdm

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_excel('/content/drive/MyDrive/bbc_data.xlsx')

print("Shape:", df.shape)
print("Columns:", df.columns.tolist())
df.head()

import re
from sklearn.preprocessing import LabelEncoder

def clean_text(text):
    text = str(text)
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"<.*?>", "", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

df['text'] = df['text'].apply(clean_text)

# Drop missing or empty rows
df = df.dropna(subset=['text', 'category'])
df = df[df['text'].str.strip() != ""]

# Encode labels
le = LabelEncoder()
df['label_id'] = le.fit_transform(df['category'])
num_classes = len(le.classes_)
print("Classes:", list(le.classes_))

from sklearn.model_selection import train_test_split

train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label_id'], random_state=42)
val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label_id'], random_state=42)

print("Train:", len(train_df), "Validation:", len(val_df), "Test:", len(test_df))

from transformers import AutoTokenizer
from datasets import Dataset

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        truncation=True,
        padding='max_length',
        max_length=256
    )

train_dataset = Dataset.from_pandas(train_df[['text', 'label_id']])
val_dataset = Dataset.from_pandas(val_df[['text', 'label_id']])
test_dataset = Dataset.from_pandas(test_df[['text', 'label_id']])

train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

train_dataset = train_dataset.rename_column("label_id", "labels")
val_dataset = val_dataset.rename_column("label_id", "labels")
test_dataset = test_dataset.rename_column("label_id", "labels")

train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
val_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

# Load model
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)

# Define metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='macro')
    return {"accuracy": acc, "f1": f1}

# Training arguments
training_args = TrainingArguments(
    output_dir="./bbc_results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    greater_is_better=True,
    logging_dir='./bbc_logs',
    logging_strategy="no",  # Changed to "no"
    report_to=None # Added to explicitly disable reporting
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

import os
os.environ["WANDB_DISABLED"] = "true"
print("WANDB logging is disabled.")

trainer.train()

metrics = trainer.evaluate(test_dataset)
print("Test set performance:")
print(metrics)

import torch

def predict_category(text, model, tokenizer, label_encoder):
    model.eval()
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=256
    ).to(model.device)
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_class_id = logits.argmax(dim=-1).item()
    predicted_label = label_encoder.inverse_transform([predicted_class_id])[0]
    return predicted_label

# Example predictions
examples = [
    "The government has announced new tax reforms for middle-class families.",
    "Manchester United defeated Chelsea 2-1 in last night's thrilling match.",
    "Apple launched its new iPhone model with improved camera features."
]

for text in examples:
    category = predict_category(text, model, tokenizer, le)
    print(f"\nüì∞ Text: {text}\n‚û°Ô∏è Predicted Category: {category}")

import joblib
from transformers import AutoModelForSequenceClassification
import inspect

# Check if 'le' is defined in the current scope
if 'le' not in locals() and 'le' not in globals():
    print("LabelEncoder 'le' not found. Please run the cell with ID 'hZ0Fb2xNsNTI' to define it before saving.")
# Check if 'model' is defined in the current scope
if 'model' not in locals() and 'model' not in globals():
    print("Model 'model' not found. Please run the cell with ID '4non4bvgsSTU' to define it before saving.")
# Check if 'tokenizer' is defined in the current scope
if 'tokenizer' not in locals() and 'tokenizer' not in globals():
    print("Tokenizer 'tokenizer' not found. Please run the cell with ID 'jY0wqVQhsQ3g' to define it before saving.")

# Proceed with saving only if le, model and tokenizer are defined
if ('le' in locals() or 'le' in globals()) and \
   ('model' in locals() or 'model' in globals()) and \
   ('tokenizer' in locals() or 'tokenizer' in globals()):
    try:
        # 1Ô∏è‚É£ Save label encoder
        joblib.dump(le, "label_encoder.pkl")

        # 2Ô∏è‚É£ Save tokenizer
        tokenizer.save_pretrained("bbc_tokenizer")

        # 3Ô∏è‚É£ Save fine-tuned model
        model.save_pretrained("bbc_model")

        print("‚úÖ Model, tokenizer, and label encoder saved successfully!")
    except Exception as e:
        print(f"An error occurred during saving: {e}")
else:
    print("Saving skipped because le, model, or tokenizer is not defined.")

import joblib

# Load the label encoder
loaded_label_encoder = joblib.load("label_encoder.pkl")

print("Label encoder loaded successfully!")
# You can now use loaded_label_encoder to inverse transform predicted labels

